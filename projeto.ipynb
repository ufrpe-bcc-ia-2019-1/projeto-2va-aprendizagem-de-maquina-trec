{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projeto.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Th_1YzQC5OX",
        "colab_type": "code",
        "outputId": "a35ace00-12a3-4d7f-9d97-821873c8a5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "trainSet1 = open(\"train5.txt\", 'r', encoding = \"ISO-8859-1\")\n",
        "\n",
        "\n",
        "categorias1 = []\n",
        "subcategorias = []\n",
        "subcategorias1 = []\n",
        "\n",
        "for line in trainSet1:\n",
        "    if(line.strip().split(':')[0] not in categorias1):\n",
        "        categorias1.append(line.strip().split(':')[0])\n",
        "    if(line.strip().split(':')[1] not in subcategorias):\n",
        "        subcategorias.append(line.strip().split(':')[1])\n",
        "        \n",
        "for element in subcategorias:\n",
        "    if(element.strip().split(' ')[0] not in subcategorias1):\n",
        "        subcategorias1.append(element.strip().split(' ')[0])\n",
        "\n",
        "print(categorias1)\n",
        "#print(subcategorias1)\n",
        "#train5 = (categorias1)\n",
        "#test5= (subcategorias1)\n",
        "\n",
        "trainSet1 = open(\"train5.txt\", 'r', encoding = \"ISO-8859-1\")\n",
        "\n",
        "\n",
        "def preprocessamento(pergunta):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(pergunta)\n",
        "    \n",
        "    tokens = [word for word in tokens if word not in categorias1 and word not in subcategorias1]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "conjuntoTreinamento1 = []\n",
        "\n",
        "for pergunta in trainSet1:\n",
        "    conjuntoTreinamento1.append(preprocessamento(pergunta))\n",
        "    \n",
        "trainSet1.close()\n",
        "trainSet1.close()\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w005q59xf29K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd653226-900e-4c92-da01-ed3ecdd6e865"
      },
      "source": [
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "trainTest = open(\"test.txt\", 'r', encoding = \"ISO-8859-1\")\n",
        "\n",
        "\n",
        "categorias1Test = []\n",
        "subcategoriasTest = []\n",
        "subcategorias1Test = []\n",
        "\n",
        "for line in trainTest:\n",
        "    if(line.strip().split(':')[0] not in categorias1):\n",
        "        categorias1Test.append(line.strip().split(':')[0])\n",
        "    if(line.strip().split(':')[1] not in subcategorias):\n",
        "        subcategoriasTest.append(line.strip().split(':')[1])\n",
        "        \n",
        "for element in subcategoriasTest:\n",
        "    if(element.strip().split(' ')[0] not in subcategorias1Test):\n",
        "        subcategorias1Test.append(element.strip().split(' ')[0])\n",
        "\n",
        "print(categorias1Test)\n",
        "#print(subcategorias1)\n",
        "#train5 = (categorias1)\n",
        "#test5= (subcategorias1)\n",
        "\n",
        "trainTest = open(\"test.txt\", 'r', encoding = \"ISO-8859-1\")\n",
        "\n",
        "\n",
        "def preprocessamento1(pergunta):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(pergunta)\n",
        "    \n",
        "    tokens = [word for word in tokens if word not in categorias1Test and word not in subcategorias1Test]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "conjuntoTreinamento1Test = []\n",
        "\n",
        "for pergunta in trainTest:\n",
        "    conjuntoTreinamento1Test.append(preprocessamento1(pergunta))\n",
        "trainTest.close()\n",
        "trainTest.close()\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vOIDYXTlloM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcbf7b09-e629-4786-912b-524f1390b373"
      },
      "source": [
        "\"\"\"\n",
        "asdaddadasd\n",
        "\n",
        "import string\n",
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "trainTest = open(\"test.txt\", 'r', encoding = \"ISO-8859-1\")\n",
        "def preprocess(text):\n",
        "  \n",
        "  # remover pontuações\n",
        "  text   = text.translate(string.punctuation)\n",
        "  \n",
        "  # converter para lowercase\n",
        "  text = text.lower()\n",
        "  \n",
        "  # tokenizar o texto em palavras\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text.lower())\n",
        "\n",
        "  # filtrar palavras\n",
        "  tokens = [word for word in tokens if word not in categorias1 and word not in subcategorias1]\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "conjuntoTreinamento1Test = []\n",
        "conjuntoTreinamento1 = []\n",
        "for doc in categorias1:  \n",
        "  conjuntoTreinamento1.append(preprocess(doc))\n",
        "  \n",
        "\n",
        "for doc in categorias1Test:\n",
        "  conjuntoTreinamento1Test.append(preprocess(doc))\n",
        "  trainTest.close()\n",
        "trainTest.close()\n",
        "print(conjuntoTreinamento1[0])\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZSeQWbp7U_Q",
        "colab_type": "code",
        "outputId": "614a0fda-467a-44da-bde5-4b8d4c88fbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "bow_model  = vectorizer.fit(conjuntoTreinamento1)\n",
        "\n",
        "X_bow_train = bow_model.transform(conjuntoTreinamento1)\n",
        "X_bow_test  = bow_model.transform(conjuntoTreinamento1Test)\n",
        "\n",
        "print(X_bow_train.shape,X_bow_test.shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5452, 8392) (500, 8392)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1koYUKMuhkxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NefL58Nuh0zX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "273e70b5-90ba-4fc9-be99-5ce3be686d7e"
      },
      "source": [
        "clf.fit(X_bow_train, conjuntoTreinamento1)\n",
        "\n",
        "acc = clf.score(X_bow_test , conjuntoTreinamento1Test)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEsDB8A1oxfV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a83d9278-36b9-4c75-94a5-1e812972b965"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=False)\n",
        "tf_model = vectorizer.fit(conjuntoTreinamento1)\n",
        "\n",
        "X_tf_train = tf_model.transform(conjuntoTreinamento1)\n",
        "X_tf_test  = tf_model.transform(conjuntoTreinamento1Test)\n",
        "\n",
        "print(X_tf_train[0,:])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAKxI_3VjJgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a357e92-2042-41a5-c422-32ca1aadad66"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "tfidf_model = vectorizer.fit(conjuntoTreinamento1)\n",
        "\n",
        "X_tfidf_train = tfidf_model.transform(conjuntoTreinamento1)\n",
        "X_tfidf_test  = tfidf_model.transform(conjuntoTreinamento1Test)\n",
        "\n",
        "print(X_tfidf_train[0,:])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxgrM4_Ajgi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15a76f23-ee1a-43a5-b453-774879ddb84e"
      },
      "source": [
        "clf.fit(X_tfidf_train, conjuntoTreinamento1)\n",
        "\n",
        "acc = clf.score(X_tfidf_test , conjuntoTreinamento1Test)\n",
        "print('Acurácia: ', acc)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A_F8E7Oiq0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5507aa4f-ef25-4be4-bff6-9cde9ef11bb4"
      },
      "source": [
        "docs = ['dist How far is it from Denver to Aspen ?']\n",
        "preprocessed_docs = [preprocessamento1(pergunta) for pergunta in docs]\n",
        "\n",
        "print('\\nafter preprocessing:')\n",
        "print(preprocessed_docs)\n",
        "\n",
        "docs_preds = clf.predict(tfidf_model.transform(preprocessed_docs))\n",
        "\n",
        "print('\\npredictions:')\n",
        "for i,pergunta in enumerate(docs):\n",
        "    print('{} -> {}'.format(doc, categorias1[docs_preds[i]]))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "after preprocessing:\n",
            "['How far is it from Denver to Aspen']\n",
            "\n",
            "predictions:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-83a10dfc1ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\npredictions:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpergunta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} -> {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorias1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocs_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not numpy.str_"
          ]
        }
      ]
    }
  ]
}